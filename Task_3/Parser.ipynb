{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "import datetime\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, Future\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# utility\n",
    "import time\n",
    "\n",
    "\n",
    "class HabrParser:\n",
    "    \"\"\"\n",
    "    Мой класс содержащий основные необходимые функции.\n",
    "    Общая идея идти назад во времени, скачивая посты и в случае достаточного размера сохранять их.\n",
    "    Pandas для хранения, tqdm для слежки за прогрессом и BeautifulSoup для парсинга.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pages = []\n",
    "        self.last_parse_date = None\n",
    "        self.log_list = []\n",
    "\n",
    "    def _big_num_fix(self, number_string: str) -> float:\n",
    "        # Строку по типу 21к привратить в число 21000.\n",
    "        if number_string[-1] == \"k\":\n",
    "            return float(number_string.replace(\",\", \".\")[:-1]) * 1000\n",
    "        return float(number_string.replace(\",\", \".\"))\n",
    "\n",
    "    def _beautiful_num_fix(self, text_string: str) -> float:\n",
    "        return float(\n",
    "            text_string.replace(\",\", \".\").replace(\"–\", \"-\").replace(\"\\xa0\", \"\")\n",
    "        )\n",
    "\n",
    "    def text_cleaner(self, text: str) -> str:\n",
    "        # Удаляются ссылки, знаки пунктуации а также \\r\\n\n",
    "        text_no_slash = text.replace(\"\\r\", \" \").replace(\"\\n\", \" \")\n",
    "        url_regex = r\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\"\n",
    "        text_no_url = re.sub(url_regex, \"\", text_no_slash)\n",
    "\n",
    "        clean_text = text_no_url.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        return clean_text\n",
    "\n",
    "    def parse_page(self, post_idx: int, min_post_size=2000) -> tuple:\n",
    "        # Скачиваю и обрабатываю страницу, возвращая tuple\n",
    "        req = requests.get(\"http://habrahabr.ru/post/\" + str(post_idx), timeout=5)\n",
    "        if req.status_code != 200:\n",
    "            return ()\n",
    "        soup = BeautifulSoup(req.text)\n",
    "\n",
    "        # Обрабатываю текст и проверяю размер, если меньше 2к то выходим\n",
    "        raw_post_text = soup.find(\"div\", {\"id\": \"post-content-body\"}).text\n",
    "        clean_text = self.text_cleaner(raw_post_text)\n",
    "        if len(clean_text) < min_post_size:\n",
    "            return ()\n",
    "        # Разбираю на части\n",
    "        date = soup.find(\"span\", {\"class\": \"post__time\"})\n",
    "        date = datetime.datetime.strptime(\n",
    "            date.get(\"data-time_published\"), \"%Y-%m-%dT%H:%MZ\"\n",
    "        )\n",
    "        title = soup.find(\"span\", {\"class\": \"post__title-text\"}).text\n",
    "\n",
    "        hubs = []\n",
    "        temp = soup.find(\n",
    "            \"ul\", {\"class\": \"inline-list inline-list_fav-tags js-post-hubs\"}\n",
    "        ).findChildren(\"a\")\n",
    "        for hub in temp:\n",
    "            hubs.append(hub.text.strip())\n",
    "\n",
    "        tags = []\n",
    "        temp = soup.find(\n",
    "            \"ul\", {\"class\": \"inline-list inline-list_fav-tags js-post-tags\"}\n",
    "        ).findChildren(\"a\")\n",
    "        for tag in temp:\n",
    "            tags.append(tag.text.strip())\n",
    "\n",
    "        # Блоки try для случаев отсутствия имени автора или титула (титула не видел но на всякий)\n",
    "        try:\n",
    "            author = soup.find(\"a\", {\"class\": \"user-info__fullname\"}).text\n",
    "        except Exception:\n",
    "            author = None\n",
    "        nickname = soup.find(\"a\", {\"class\": \"user-info__nickname\"}).text\n",
    "\n",
    "        try:\n",
    "            author_title = soup.find(\"div\", {\"class\": \"user-info__specialization\"}).text\n",
    "        except Exception:\n",
    "            author_title = None\n",
    "\n",
    "        # Карма и рэйтинг типа int\n",
    "        try:\n",
    "            author_karma = self._beautiful_num_fix(\n",
    "                soup.find(\"a\", {\"class\": \"user-info__stats-item stacked-counter\"})\n",
    "                .contents[1]\n",
    "                .text\n",
    "            )\n",
    "        except Exception:\n",
    "            author_karma = None\n",
    "        try:\n",
    "            author_rating = self._beautiful_num_fix(\n",
    "                soup.find(\n",
    "                    \"a\",\n",
    "                    {\n",
    "                        \"class\": \"user-info__stats-item stacked-counter stacked-counter_rating\"\n",
    "                    },\n",
    "                )\n",
    "                .contents[1]\n",
    "                .text\n",
    "            )\n",
    "        except Exception:\n",
    "            author_rating = None\n",
    "\n",
    "        temp = soup.find(\"div\", {\"class\": \"voting-wjt voting-wjt_post js-post-vote\"})\n",
    "        post_rating = self._beautiful_num_fix(temp.findChildren(\"span\")[0].text)\n",
    "\n",
    "        times_saved = self._big_num_fix(\n",
    "            soup.find(\"span\", {\"class\": \"bookmark__counter js-favs_count\"}).text\n",
    "        )\n",
    "        views = self._big_num_fix(\n",
    "            soup.find(\"span\", {\"class\": \"post-stats__views-count\"}).text\n",
    "        )\n",
    "\n",
    "        # Получаем количество комментариев, с проверкой на слово \"Комментировать\", что равносильно 0\n",
    "        temp = soup.find(\"span\", {\"id\": \"post-stats-comments-count\"}).text\n",
    "        if temp == \"Комментировать\":\n",
    "            comment_count = 0\n",
    "        else:\n",
    "            comment_count = self._big_num_fix(temp)\n",
    "\n",
    "        return (\n",
    "            post_idx,\n",
    "            author,\n",
    "            nickname,\n",
    "            author_title,\n",
    "            author_karma,\n",
    "            author_rating,\n",
    "            title,\n",
    "            date,\n",
    "            hubs,\n",
    "            tags,\n",
    "            post_rating,\n",
    "            times_saved,\n",
    "            views,\n",
    "            comment_count,\n",
    "            raw_post_text,\n",
    "            clean_text,\n",
    "        )\n",
    "\n",
    "    def _threaded_parse_page(self, post_idx: int, min_post_size=2000) -> tuple:\n",
    "        # log_file = None ONLY beacause it's always given the log and order of arguments\n",
    "        page_log = None\n",
    "        try:\n",
    "            post = self.parse_page(post_idx, min_post_size)\n",
    "        except TimeoutError:\n",
    "            print(\"Timeout Error\\n\")\n",
    "            #print(post_idx)\n",
    "            post = ()\n",
    "            page_log = str(post_idx) + \"\\n\"\n",
    "        except:\n",
    "            # в случае ошибок (не считая проблем с запросами, напримем 404) сохраняю индекс в лог\n",
    "            #print(post_idx)\n",
    "            post = ()\n",
    "            page_log = str(post_idx) + \"\\n\"\n",
    "        return (post, page_log)\n",
    "\n",
    "    def multi_run(\n",
    "        self,\n",
    "        number_of_pages=100000,\n",
    "        starting_idx=540000,\n",
    "        min_post_size=2000,\n",
    "        time_between=0.5,\n",
    "        num_of_processes=10,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Многопоточный парсинг.\n",
    "        Парсим number_of_pages страниц двигаясь вниз по индексу с номера starting_idx с\n",
    "        паузами time_between секунд.\n",
    "        Все данные сохранённые в классе будут УДАЛЕНЫ.\n",
    "\n",
    "        \"\"\"\n",
    "        self.pages = []\n",
    "        self.log_list = []\n",
    "        # Индекст страницы для скачивания\n",
    "        page_idx = starting_idx\n",
    "        # Время для создания лог-файла и названия файла сохранения\n",
    "        now = datetime.datetime.now()\n",
    "        self.last_parse_date = now\n",
    "        # создаю tqdm\n",
    "        t = tqdm(total=number_of_pages)\n",
    "        # создаю пул потоков\n",
    "        executor = ThreadPoolExecutor(max_workers=num_of_processes)\n",
    "        # счётчик подходящих страниц (раньше использовался напрямую tqdm, но во избежания ошибок интеграции заменён)\n",
    "        counter = 0\n",
    "        while counter < number_of_pages:\n",
    "            # создаю временные переменные для обработки потоков и их результатов\n",
    "            post = []\n",
    "            futures = []\n",
    "            for thr in range(num_of_processes):\n",
    "                futures.append(\n",
    "                    executor.submit(\n",
    "                        self._threaded_parse_page, page_idx - thr, min_post_size\n",
    "                    )\n",
    "                )\n",
    "            for thr in range(num_of_processes):\n",
    "                res = futures[thr].result()\n",
    "                # проверяю что нашлось. () - неподходящие данные или несуществующая страница\n",
    "                # и если есть код страницы то сохраняю его в лог (а страница удаляется, т.к. ошибка)\n",
    "                if res[0] != ():\n",
    "                    post.append(res[0])\n",
    "\n",
    "                elif res[1] is not None:\n",
    "                    self.log_list.append(res[1])\n",
    "\n",
    "                sleep(time_between)\n",
    "            # сохраняю результаты и обновляю индекс\n",
    "            self.pages.extend(post)\n",
    "            t.update(len(post))\n",
    "            counter += len(post)\n",
    "            page_idx -= num_of_processes\n",
    "\n",
    "        # по окончании вывожу дополнительную информацию в лог и консоль\n",
    "        final_message = f\"\"\"\n",
    "        Finished!\n",
    "        Total pages: {len(self.pages)}\n",
    "        Starting index: {starting_idx}\n",
    "        Final index: {page_idx}\n",
    "        Starting time : {now.strftime(\"%d/%m/%y--%H:%M:%S\")}\n",
    "        Ending time: {datetime.datetime.now().strftime(\"%d/%m/%y--%H:%M:%S\")}\"\"\"\n",
    "\n",
    "        self.log_list.append(final_message)\n",
    "        print(final_message)\n",
    "\n",
    "    def save(self, save_path=\"data/\"):\n",
    "        \"\"\"\n",
    "\n",
    "        Сохранить скаченные данные в DataFrame picklе, а также сохранить лог в файл.\n",
    "        Если данных нет, то выводится сообщение.\n",
    "\n",
    "        \"\"\"\n",
    "        if self.pages == 0 or self.last_parse_date is None:\n",
    "            print(\"No data to save\")\n",
    "        else:\n",
    "            df = pd.DataFrame(self.pages)\n",
    "            df = df.set_axis(\n",
    "                [\n",
    "                    \"post_idx\",\n",
    "                    \"author\",\n",
    "                    \"nickname\",\n",
    "                    \"author_title\",\n",
    "                    \"author_karma\",\n",
    "                    \"author_rating\",\n",
    "                    \"title\",\n",
    "                    \"date\",\n",
    "                    \"hubs\",\n",
    "                    \"tags\",\n",
    "                    \"post_rating\",\n",
    "                    \"times_saved\",\n",
    "                    \"views\",\n",
    "                    \"comment_count\",\n",
    "                    \"raw_post_text\",\n",
    "                    \"clean_text\",\n",
    "                ],\n",
    "                axis=1,\n",
    "            )\n",
    "            name = (\n",
    "                \"habr_\" + self.last_parse_date.strftime(\"%d_%m_%y--%H_%M_%S\") + \".pkl\"\n",
    "            )\n",
    "            df.to_pickle(save_path + name)\n",
    "            # сохранение логов\n",
    "            log = open(\n",
    "                save_path\n",
    "                + \"habr_logs_\"\n",
    "                + self.last_parse_date.strftime(\"%d_%m_%y--%H_%M_%S\")\n",
    "                + \".txt\",\n",
    "                \"w\",\n",
    "            )\n",
    "            log.write(\"\".join(self.log_list))\n",
    "            log.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
